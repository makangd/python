{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatial Interpolation\n",
    "\n",
    "<img src=\"images/interpol.png\" width=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spatial interpolation is the process of using observations with known values to estimate values at unkown locations.For example if we have precipitation data from different weather stations and we want to create a continous map for the entire area. Fortunatley in this case we can generally safely assume that values at nearby locations will be similar (spatial autocorrelation).\n",
    "\n",
    "There are many interpolation tools available, but these tools can usually be grouped into two categories: \n",
    "\n",
    "- deterministic: using mathematical functions, based on either the extent of similarity (IDW) or the degree of smoothing (RBF)\n",
    "- geostatistical: use both mathematical and statistical methods to predict values at all locations within region of interest and to provide probabilistic estimates of the quality of the interpolation based on the spatial autocorrelation among data points\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Temperature in Burkina Faso\n",
    "\n",
    "In this example we will take temperature data from different weather stations and try to interpolate the missing data in order to create a map showing the mean temperature in Burkina Faso for 2019. The data set we are using contains measurements from weather stations located in Burkina Faso and the neighbouring countries for the year 2019. Station data is derived from the GSOD (Global Surface Summary of the Day) dataset. \n",
    "Global Surface Summary of the Day is derived from The Integrated Surface Hourly (ISH) dataset. The ISH dataset includes global data obtained from the USAF Climatology Center, located in the Federal Climate Complex with NCDC. The latest daily summary data are normally available 1-2 days after the date-time of the observations used in the daily summaries. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ws = gpd.read_file('Data/vector/ws.shp')\n",
    "region = gpd.read_file('Data/vector/reg.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = ws.round(6)\n",
    "ws['coords'] = ws['geometry'].apply(lambda x: x.representative_point().coords[:])\n",
    "ws['coords'] = [coords[0] for coords in ws['coords']]\n",
    "ws.plot(figsize=(10,10))\n",
    "for idx, row in ws.iterrows():\n",
    "    plt.annotate(s=row['mean_temp'], xy=row['coords'],\n",
    "                 horizontalalignment='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import mplleaflet\n",
    "\n",
    "ax = ws.plot(column='mean_temp', colormap='RdBu_r')\n",
    "region.plot(ax=ax)\n",
    "mplleaflet.display(fig=ax.figure, tiles='cartodb_positron')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deterministic methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this course we will have a look at 2 deterministic methods: \n",
    "- Thiessen polygons \n",
    "- Inverse Distance Weighted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voronoi polygons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Voronoi diagram, also called Thiessen polygons or Dirichlet decomposition, is a decomposition of space into regions determined by a given set of points in space, here called centers. Each region is defined by exactly one centre and includes all points of space which, in terms of Euclidean metrics, are closer to the centre of the region than to any other centre. Such regions are also called Voronoi regions. The Voronoi diagram is formed from all points that have more than one nearest centre and thus form the boundaries of the regions. \n",
    "\n",
    "\n",
    "<img src=\"images/voronoi.png\" width=200 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas\n",
    "import geoplot\n",
    "# Import shapely to convert string lat-longs to Point objects\n",
    "from shapely.geometry import Point\n",
    "\n",
    "ax = geoplot.voronoi(ws,  # Define the GeoPandas DataFrame\n",
    "                     hue='mean_temp',  # df column used to color regions\n",
    "                     clip=region,  # Define the voronoi clipping (map edge)\n",
    "                     #projection=proj,  # Define the Projection\n",
    "                     cmap='RdBu_r',  # color set  # No. of discretized buckets to create\n",
    "                     legend=True, # Create a legend\n",
    "                     edgecolor='white',  # Color of the voronoi boundaries\n",
    "                     linewidth=0.01  # width of the voronoi boundary lines\n",
    "                    )\n",
    "geoplot.polyplot(region,  # Base Map\n",
    "                 ax=ax,  # Axis attribute we created above\n",
    "                 extent=region.total_bounds,  # Set plotting boundaries to base map boundaries\n",
    "                 edgecolor='black',  # Color of base map's edges\n",
    "                 linewidth=1,  # Width of base map's edge lines\n",
    "                 zorder=1  # Plot base map edges above the voronoi regions\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from libpysal.cg.voronoi  import voronoi, voronoi_frames\n",
    "\n",
    "x = ws['geometry'].apply(lambda x: x.x)\n",
    "y = ws['geometry'].apply(lambda x: x.y)\n",
    "data=np.column_stack((x, y))\n",
    "region_df, point_df = voronoi_frames(data)\n",
    "region_df['mean_temp'] = ws['mean_temp']\n",
    "region_df.plot(column='mean_temp', colormap='RdBu_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vo_clip = gpd.clip(region_df,region)\n",
    "vo_clip.plot(column='mean_temp', colormap='RdBu_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = ws.plot(column='mean_temp', colormap='RdBu_r')\n",
    "vo_clip.plot(ax=ax, column='mean_temp', colormap='RdBu_r')\n",
    "mplleaflet.display(fig=ax.figure, tiles='cartodb_positron')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IDW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "Probably one of the oldest spatial prediction technique is the inverse distance interpolation. IDW is a simple non-statistical interpolation method and is based on the principle of spatial correlation - i.e. things that are closer to each other are more similar in value. IDW uses only the distance between the individual measuring points and the point to be calculated as the influencing factor for the interpolation, i.e. the greater the distance, the smaller the influence of the measuring point on the calculation of a value for a non-sampled location.\n",
    "\n",
    "\n",
    "\n",
    "$$\\hat{P} = \\frac{\\sum_{i=1}^n \\frac{P_i}{d_i²}}{\\sum_{i=1}^n\\frac{1}{d_i²}}$$\n",
    "\n",
    "The measure closest to the prediction location have more influence on the predicted value than those farther away. IDW assumes that each measurement influences the surrounding and that influence becomes weaker with increasing distance. The idea is give greater weights to observations close to the location we want to predict. With increasing distance to the observation the smaller weights are assigned, hence the name inverse distance weighted\n",
    "\n",
    "<img src=\"images/idw_calc.png\"  width=120 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "$$\\hat{P} = \\frac{\\frac{10}{25²} + \\frac{20}{15²} +\\frac{30}{10²}}{\\sum_{i=1}^n\\frac{1}{25²}\\frac{1}{15²}\\frac{1}{10²}} = 25.24$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skspatial\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ulc = (region.bounds.minx.min(),region.bounds.maxy.max())\n",
    "lrc = (region.bounds.maxx.max(),region.bounds.miny.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = 00.1\n",
    "ml = skspatial.interp2d(ws,'mean_temp',res=res, ulc=ulc, lrc=lrc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = ml.knn_2D(k=10, weights='distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = ml.plot_image(array)\n",
    "CS = plt.contour(np.flipud(array),extent=ml.extent)\n",
    "region.boundary.plot(ax=ax, color='red')\n",
    "plt.clabel(CS, inline=1,\n",
    "           fmt='%1.1f',\n",
    "           fontsize=14)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Statistical Approach "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trend Surfaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This interpolation methods fits a polynomial surface by least-squares regression through the sample data points.The result is a surface that minimizes the variance of the surface in relation to the input values. In this section we will explore a 0th order, 1st order and 2nd order surface trend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/surface_trend.png\" width=600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first order surface polynomial is a flat plane whose formula is given by: \n",
    "$$Z = a + bX + cY$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding a term to the mathematical formula allows us to add another bend in the plane.\n",
    "- first-order polynomial (linear).\n",
    "<img src=\"images/surface_trend1.png\" width=150 />\n",
    "- second-order polynomial (quadratic) \n",
    "<img src=\"images/surface_trend2.png\" width=150 />\n",
    "- third-order (cubic)\n",
    "<img src=\"images/surface_trend3.png\" width=150 />\n",
    "- fourth-oder (quartic)\n",
    "<img src=\"images/surface_trend4.png\" width=150 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cartopy.crs as ccrs\n",
    "import verde as vd\n",
    "\n",
    "ws['longitude'] = ws['geometry'].apply(lambda x: x.x)\n",
    "ws['latitude'] = ws['geometry'].apply(lambda x: x.y)\n",
    "\n",
    "coordinates = (ws.longitude.values, ws.latitude.values)\n",
    "\n",
    "trend = vd.Trend(degree=3).fit(coordinates, ws.mean_temp)\n",
    "\n",
    "ws[\"trend\"] = trend.predict(coordinates)\n",
    "ws[\"residual\"] = ws.mean_temp - ws.trend\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = trend.grid(spacing=0.1, dims=[\"latitude\", \"longitude\"])\n",
    "grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "ax = plt.axes(projection=ccrs.Mercator())\n",
    "ax.set_title(\"Temperature gridded\")\n",
    "ax.plot(*coordinates, \".k\", markersize=1, transform=ccrs.PlateCarree())\n",
    "tmp = ax.pcolormesh(\n",
    "    grid.longitude,\n",
    "    grid.latitude,\n",
    "    grid.scalars,\n",
    "    cmap=\"RdBu_r\",\n",
    "    transform=ccrs.PlateCarree(),\n",
    ")\n",
    "plt.colorbar(tmp).set_label(\"temperature (C)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = ws.mean_temp - ws.trend\n",
    "fig, axes = plt.subplots(\n",
    "    1, 2, figsize=(10, 6), subplot_kw=dict(projection=ccrs.Mercator())\n",
    ")\n",
    "\n",
    "ax = axes[0]\n",
    "ax.set_title(\"Trend\")\n",
    "tmp = ax.scatter(\n",
    "    ws.longitude,\n",
    "    ws.latitude,\n",
    "    c=ws.trend,\n",
    "    s=60,\n",
    "    cmap=\"RdBu_r\",\n",
    "    transform=ccrs.PlateCarree(),\n",
    ")\n",
    "plt.colorbar(tmp, ax=ax, orientation=\"horizontal\", pad=0.06)\n",
    "\n",
    "ax = axes[1]\n",
    "ax.set_title(\"Residuals\")\n",
    "tmp = ax.scatter(\n",
    "    ws.longitude,\n",
    "    ws.latitude,\n",
    "    c=residuals,\n",
    "    cmap=\"RdBu_r\",\n",
    "    transform=ccrs.PlateCarree(),\n",
    ")\n",
    "plt.colorbar(tmp, ax=ax, orientation=\"horizontal\", pad=0.08)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = vd.train_test_split(\n",
    "    coordinates, ws.mean_temp, test_size=0.2, random_state=0,\n",
    ")\n",
    "\n",
    "trend = vd.Trend(degree=3).fit(*train)\n",
    "\n",
    "test_values = np.array(list(test[1]))\n",
    "prediction = trend.predict(test[0])\n",
    "\n",
    "df = pd.DataFrame({'obs':test_values[0], 'pred':prediction})\n",
    "\n",
    "correlation_matrix = np.corrcoef(test_values[0], prediction)\n",
    "correlation_xy = correlation_matrix[0,1]\n",
    "r_squared = correlation_xy**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "p = sns.lmplot(x='obs',y='pred',data=df,\n",
    "        line_kws={'label':\"Linear Reg\"}, legend=True)\n",
    "\n",
    "ax = p.axes[0, 0]\n",
    "ax.legend()\n",
    "leg = ax.get_legend()\n",
    "L_labels = leg.get_texts()\n",
    "label_line_2 = r'$R^2:{0:.2f}$'.format(r_squared) \n",
    "L_labels[0].set_text(label_line_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = vd.cross_val_score(vd.Trend(degree=3), coordinates, ws.mean_temp)\n",
    "print(\"k-fold scores:\", scores)\n",
    "print(\"Mean score:\", np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spline inerpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The spline method works a little bit like the trend surface. You can think of a rubber disc passing through the points, which is bent while the overall curvature of the surface is minimized. It adjusts a mathematical function according to a specified number of next input points as it passes through the reference points. This method is best suited for generating smoothly changing surfaces, e.g. heights, water levels or pollutant concentrations.\n",
    "\n",
    "The basic form of the spline method with minimal curvature imposes the following two conditions on the interpolant:\n",
    "\n",
    "   - The surface must pass exactly through the data points.\n",
    "   - The surface must have minimal curvature. The total sum of the squares of the second derivative values of the surface, measured over all points of the surface, must be a minimum.\n",
    "\n",
    "<img src=\"images/Spline_interpolation.png\" width=600 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinates = (ws.longitude.values, ws.latitude.values)\n",
    "spline = vd.Spline().fit(coordinates, ws.mean_temp)\n",
    "grid = spline.grid(spacing=0.1, dims=[\"latitude\", \"longitude\"])\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "ax = plt.axes(projection=ccrs.Mercator())\n",
    "ax.set_title(\"Temperature gridded\")\n",
    "ax.plot(*coordinates, \".k\", markersize=1, transform=ccrs.PlateCarree())\n",
    "tmp = ax.pcolormesh(\n",
    "    grid.longitude,\n",
    "    grid.latitude,\n",
    "    grid.scalars,\n",
    "    cmap=\"RdBu_r\",\n",
    "    transform=ccrs.PlateCarree(),\n",
    ")\n",
    "plt.colorbar(tmp).set_label(\"temperature (C)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = vd.train_test_split(\n",
    "    coordinates, ws.mean_temp, test_size=0.2, random_state=0,\n",
    ")\n",
    "\n",
    "spline = vd.Spline().fit(*train)\n",
    "\n",
    "test_values = np.array(list(test[1]))\n",
    "prediction = spline.predict(test[0])\n",
    "\n",
    "df = pd.DataFrame({'obs':test_values[0], 'pred':prediction})\n",
    "\n",
    "correlation_matrix = np.corrcoef(test_values[0], prediction)\n",
    "correlation_xy = correlation_matrix[0,1]\n",
    "r_squared = correlation_xy**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sns.lmplot(x='obs',y='pred',data=df,\n",
    "        line_kws={'label':\"Linear Reg\"}, legend=True)\n",
    "\n",
    "ax = p.axes[0, 0]\n",
    "ax.legend()\n",
    "leg = ax.get_legend()\n",
    "L_labels = leg.get_texts()\n",
    "label_line_2 = r'$R^2:{0:.2f}$'.format(r_squared) \n",
    "L_labels[0].set_text(label_line_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = vd.cross_val_score(vd.Spline(), coordinates, ws.mean_temp)\n",
    "print(\"k-fold scores:\", scores)\n",
    "print(\"Mean score:\", np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kriging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Kriging method assumes that the distance or direction between reference points reflects a spatial correlation that can be used to explain variations on the surface. The Kriging tool fits a mathematical function to a specified number of points or all points within a specified radius to determine the output value for each location. \n",
    "\n",
    "\"Kriging\" is a multi-step process that includes: preliminary statistical analysis of the data, variogram modeling, creation of the surface, and (optionally) examination of a variance surface. In geostatistics the spatial correlation is modelled by the variogram instead of a correlogram or covariogram, largely for historical reasons. \n",
    "\n",
    "The main idea of kriging is that close sample points get more weight in a prediction to improve the estimate.  Kriging relies on the knowledge of some kind of spatial structure, which is modeled via the second-order properties, i.e.  variogram or covariance, of the underlying random function Z(x).Thus, the kriging process is best used when you know that the data has a spatially correlated distance or directional trend.Several forms of kriging interpolators exist (e.g. ordinary, universal and simple). A standard version of kriging is called ordinary kriging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\gamma (h) = \\frac{1}{2N(h)} * \\sum_{i=1}^{N(h)}(Z(x_i) - Z(x_{i + h}))^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/variogram.png\" width=500 />\n",
    "\n",
    "   - Sill: The value at which the model first flattens out. \n",
    "   - Range: The distance at which the model first flattens out\n",
    "   - Nugget: The value at which the semi-variogram (almost) intercepts the y-value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "from pykrige.ok import OrdinaryKriging\n",
    "from pykrige.kriging_tools import write_asc_grid\n",
    "import pykrige.kriging_tools as kt\n",
    "import matplotlib.pyplot as plt\n",
    "#from mpl_toolkits.basemap import Basemap\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from matplotlib.patches import Path, PathPatch\n",
    "\n",
    "region = gpd.read_file('Data/vector/reg.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.ops import cascaded_union\n",
    "boundary = cascaded_union(region.geometry)\n",
    "xmin, ymin, xmax, ymax = boundary.bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmin = xmin-0.01\n",
    "xmax = xmax+0.01\n",
    "\n",
    "ymin = ymin-0.01\n",
    "ymax = ymax+0.01\n",
    "\n",
    "grid_lon = np.linspace(xmin, xmax, 100)\n",
    "grid_lat = np.linspace(ymin, ymax, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OK = OrdinaryKriging(ws.longitude,ws.latitude,ws.mean_temp, variogram_model='gaussian', verbose=True, enable_plotting=True,nlags=20)\n",
    "z1, ss1 = OK.execute('grid', grid_lon, grid_lat)\n",
    "print (z1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xintrp, yintrp = np.meshgrid(grid_lon, grid_lat) \n",
    "fig, ax = plt.subplots(figsize=(30,30))\n",
    "\n",
    "boundarygeom = region.geometry\n",
    "contour = plt.contourf(xintrp, yintrp, z1,len(z1),cmap=plt.cm.jet,alpha = 0.8) \n",
    "plt.colorbar(contour)\n",
    "\n",
    "region.plot(ax=ax, color='white', alpha = 0.2, linewidth=5.5, edgecolor='black', zorder = 5)\n",
    "npts = len(ws.longitude)\n",
    "\n",
    "plt.scatter(ws.longitude, ws.latitude,marker='o',c='b',s=npts)\n",
    "\n",
    "plt.xlim(xmin,xmax)\n",
    "plt.ylim(ymin,ymax)\n",
    "\n",
    "plt.xticks(fontsize = 30, rotation=60)\n",
    "plt.yticks(fontsize = 30)\n",
    "\n",
    "#Tempreture\n",
    "plt.title('Spatial interpolation from temperature (%d points)' % npts,fontsize = 40)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "test_indexes = np.random.choice(a=ws.index, size=int(np.round(len(ws.index.values)/4)))\n",
    "train_indexes = [index for index in ws.index if index not in test_indexes]\n",
    "ws_test = ws.loc[test_indexes,:].copy()\n",
    "ws_train = ws.loc[train_indexes,:].copy()\n",
    "print('Number of observations in training: {}, in test: {}'.format(len(ws_train), len(ws_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OrdinaryKriging(ws_train.longitude,ws_train.latitude,ws_train.mean_temp, variogram_model='gaussian', verbose=True, enable_plotting=True,nlags=20)\n",
    "ws_test['prediction'] = model.execute(style='points', xpoints=ws_test.longitude, ypoints=ws_test.latitude )[0].data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'obs':ws_test['mean_temp'] , 'pred':ws_test['prediction']})\n",
    "\n",
    "correlation_matrix = np.corrcoef(ws_test['mean_temp'], ws_test['prediction'])\n",
    "correlation_xy = correlation_matrix[0,1]\n",
    "r_squared = correlation_xy**2\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sns.lmplot(x='obs',y='pred',data=df,\n",
    "        line_kws={'label':\"Linear Reg\"}, legend=True)\n",
    "ax = p.axes[0, 0]\n",
    "ax.legend()\n",
    "leg = ax.get_legend()\n",
    "L_labels = leg.get_texts()\n",
    "label_line_2 = r'$R^2:{0:.2f}$'.format(r_squared) \n",
    "L_labels[0].set_text(label_line_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Literatur\n",
    "\n",
    "Bivand, Roger S., et al. Applied spatial data analysis with R. Vol. 747248717. New York: Springer, 2008.\n",
    "\n",
    "https://rspatial.org/raster/analysis/\n",
    "\n",
    "https://mgimond.github.io/Spatial/introGIS.html\n",
    "\n",
    "https://mmaelicke.github.io/scikit-gstat/tutorials/01_getting_started.html\n",
    "\n",
    "https://github.com/WZBSocialScienceCenter/geovoronoi\n",
    "\n",
    "https://github.com/rosskush/skspatial\n",
    "\n",
    "https://github.com/fatiando/verde"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
